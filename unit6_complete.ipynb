{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkf87/hf_RL_course_ex/blob/main/unit6_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 6: Advantage Actor Critic (A2C) using Robotics Simulations with PyBullet and Panda-Gym ü§ñ\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/thumbnail.png\"  alt=\"Thumbnail\"/>\n",
        "\n",
        "In this notebook, you'll learn to use A2C with PyBullet and Panda-Gym, two set of robotics environments. \n",
        "\n",
        "With [PyBullet](https://github.com/bulletphysics/bullet3), you're going to **train a robot to move**:\n",
        "- `AntBulletEnv-v0` üï∏Ô∏è More precisely, a spider (they say Ant but come on... it's a spider üòÜ) üï∏Ô∏è\n",
        "\n",
        "Then, with [Panda-Gym](https://github.com/qgallouedec/panda-gym), you're going **to train a robotic arm** (Franka Emika Panda robot) to perform a task:\n",
        "- `Reach`: the robot must place its end-effector at a target position.\n",
        "\n",
        "After that, you'll be able **to train in other robotics environments**.\n"
      ],
      "metadata": {
        "id": "-PTReiOw-RAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/environments.gif\" alt=\"Robotics environments\"/>"
      ],
      "metadata": {
        "id": "2VGL_0ncoAJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéÆ Environments: \n",
        "\n",
        "- [PyBullet](https://github.com/bulletphysics/bullet3)\n",
        "- [Panda-Gym](https://github.com/qgallouedec/panda-gym)\n",
        "\n",
        "###üìö RL-Library: \n",
        "\n",
        "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)"
      ],
      "metadata": {
        "id": "QInFitfWno1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ],
      "metadata": {
        "id": "2CcdX4g3oFlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "\n",
        "At the end of the notebook, you will:\n",
        "\n",
        "- Be able to use **PyBullet** and **Panda-Gym**, the environment libraries.\n",
        "- Be able to **train robots using A2C**.\n",
        "- Understand why **we need to normalize the input**.\n",
        "- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MoubJX20oKaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook is from the Deep Reinforcement Learning Course\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>\n",
        "\n",
        "In this free course, you will:\n",
        "\n",
        "- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n",
        "- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n",
        "- ü§ñ Train **agents in unique environments** \n",
        "\n",
        "And more check üìö the syllabus üëâ https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
        "\n",
        "\n",
        "The best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5"
      ],
      "metadata": {
        "id": "DoUNkTExoUED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving into the notebook, you need to:\n",
        "\n",
        "üî≤ üìö Study [Actor-Critic methods by reading Unit 6](https://huggingface.co/deep-rl-course/unit6/introduction) ü§ó  "
      ],
      "metadata": {
        "id": "BTuQAUAPoa5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's train our first robots ü§ñ"
      ],
      "metadata": {
        "id": "iajHvVDWoo01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),  you need to push your two trained models to the Hub and get the following results:\n",
        "\n",
        "- `AntBulletEnv-v0` get a result of >= 650.\n",
        "- `PandaReachDense-v2` get a result of >= -3.5.\n",
        "\n",
        "To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) and find your model, **the result = mean_reward - std of reward**\n",
        "\n",
        "If you don't find your model, **go to the bottom of the page and click on the refresh button**\n",
        "\n",
        "For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ],
      "metadata": {
        "id": "zbOENTE2os_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the GPU üí™\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ],
      "metadata": {
        "id": "PU4FVzaoM6fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ],
      "metadata": {
        "id": "KV0NyFdQM9ZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a virtual display üîΩ\n",
        "\n",
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames). \n",
        "\n",
        "Hence the following cell will install the librairies and create and run a virtual screen üñ•"
      ],
      "metadata": {
        "id": "bTpYcVZVMzUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "ww5PQH1gNLI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies üîΩ\n",
        "The first step is to install the dependencies, we‚Äôll install multiple ones:\n",
        "\n",
        "- `pybullet`: Contains the walking robots environments.\n",
        "- `panda-gym`: Contains the robotics arm environments.\n",
        "- `stable-baselines3[extra]`: The SB3 deep reinforcement learning library.\n",
        "- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ü§ó Hub.\n",
        "- `huggingface_hub`: Library allowing anyone to work with the Hub repositories."
      ],
      "metadata": {
        "id": "e1obkbdJ_KnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yZRi_0bQGPM"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit6/requirements-unit6.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the packages üì¶"
      ],
      "metadata": {
        "id": "QTep3PQQABLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pybullet_envs\n",
        "import panda_gym\n",
        "import gym\n",
        "\n",
        "import os\n",
        "\n",
        "from huggingface_sb3 import load_from_hub, package_to_hub\n",
        "\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "HpiB8VdnQ7Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment 1: AntBulletEnv-v0 üï∏\n",
        "\n"
      ],
      "metadata": {
        "id": "lfBwIS_oAVXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the AntBulletEnv-v0\n",
        "#### The environment üéÆ\n",
        "In this environment, the agent needs to use correctly its different joints to walk correctly.\n",
        "You can find a detailled explanation of this environment here: https://hackmd.io/@jeffreymo/SJJrSJh5_#PyBullet"
      ],
      "metadata": {
        "id": "frVXOrnlBerQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"AntBulletEnv-v0\"\n",
        "# Create the env\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space"
      ],
      "metadata": {
        "id": "JpU-JCDQYYax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "id": "2ZfvcCqEYgrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The observation Space (from [Jeffrey Y Mo](https://hackmd.io/@jeffreymo/SJJrSJh5_#PyBullet)):\n",
        "\n",
        "The difference is that our observation space is 28 not 29.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/obs_space.png\" alt=\"PyBullet Ant Obs space\"/>\n"
      ],
      "metadata": {
        "id": "QzMmsdMJS7jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "id": "Tc89eLTYYkK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The action Space (from [Jeffrey Y Mo](https://hackmd.io/@jeffreymo/SJJrSJh5_#PyBullet)):\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/action_space.png\" alt=\"PyBullet Ant Obs space\"/>\n"
      ],
      "metadata": {
        "id": "3RfsHhzZS9Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize observation and rewards"
      ],
      "metadata": {
        "id": "S5sXcg469ysB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good practice in reinforcement learning is to [normalize input features](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html). \n",
        "\n",
        "For that purpose, there is a wrapper that will compute a running average and standard deviation of input features.\n",
        "\n",
        "We also normalize rewards with this same wrapper by adding `norm_reward = True`\n",
        "\n",
        "[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)"
      ],
      "metadata": {
        "id": "1ZyX6qf3Zva9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_vec_env(env_id, n_envs=4)\n",
        "\n",
        "# Adding this wrapper to normalize the observation and the reward\n",
        "env = # TODO: Add the wrapper"
      ],
      "metadata": {
        "id": "1RsDtHHAQ9Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "tF42HvI7-gs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_vec_env(env_id, n_envs=4)\n",
        "\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
      ],
      "metadata": {
        "id": "2O67mqgC-hol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the A2C Model ü§ñ\n",
        "\n",
        "In this case, because we have a vector of 28 values as input, we'll use an MLP (multi-layer perceptron) as policy.\n",
        "\n",
        "For more information about A2C implementation with StableBaselines3 check: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes\n",
        "\n",
        "To find the best parameters I checked the [official trained agents by Stable-Baselines3 team](https://huggingface.co/sb3)."
      ],
      "metadata": {
        "id": "4JmEVU6z1ZA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = # Create the A2C model and try to find the best parameters"
      ],
      "metadata": {
        "id": "vR3T4qFt164I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "nWAuOOLh-oQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = A2C(policy = \"MlpPolicy\",\n",
        "            env = env,\n",
        "            gae_lambda = 0.9,\n",
        "            gamma = 0.99,\n",
        "            learning_rate = 0.00096,\n",
        "            max_grad_norm = 0.5,\n",
        "            n_steps = 8,\n",
        "            vf_coef = 0.4,\n",
        "            ent_coef = 0.0,\n",
        "            policy_kwargs=dict(\n",
        "            log_std_init=-2, ortho_init=False),\n",
        "            normalize_advantage=False,\n",
        "            use_rms_prop= True,\n",
        "            use_sde= True,\n",
        "            verbose=1)"
      ],
      "metadata": {
        "id": "FKFLY54T-pU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the A2C agent üèÉ\n",
        "- Let's train our agent for 2,000,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~25-40min"
      ],
      "metadata": {
        "id": "opyK3mpJ1-m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(2_000_000)"
      ],
      "metadata": {
        "id": "4TuGHZD7RF1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and  VecNormalize statistics when saving the agent\n",
        "model.save(\"a2c-AntBulletEnv-v0\")\n",
        "env.save(\"vec_normalize.pkl\")"
      ],
      "metadata": {
        "id": "MfYtjj19cKFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the agent üìà\n",
        "- Now that's our  agent is trained, we need to **check its performance**.\n",
        "- Stable-Baselines3 provides a method to do that: `evaluate_policy`\n",
        "- In my case, I got a mean reward of `2371.90 +/- 16.50`"
      ],
      "metadata": {
        "id": "01M9GCd32Ig-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "# Load the saved statistics\n",
        "eval_env = DummyVecEnv([lambda: gym.make(\"AntBulletEnv-v0\")])\n",
        "eval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n",
        "\n",
        "#  do not update them at test time\n",
        "eval_env.training = False\n",
        "# reward normalization is not needed at test time\n",
        "eval_env.norm_reward = False\n",
        "\n",
        "# Load the agent\n",
        "model = A2C.load(\"a2c-AntBulletEnv-v0\")\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "liirTVoDkHq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Publish your trained model on the Hub üî•\n",
        "Now that we saw we got good results after the training, we can publish our trained model on the Hub with one line of code.\n",
        "\n",
        "üìö The libraries documentation üëâ https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20\n",
        "\n",
        "Here's an example of a Model Card (with a PyBullet environment):\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/modelcardpybullet.png\" alt=\"Model Card Pybullet\"/>"
      ],
      "metadata": {
        "id": "44L9LVQaavR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using `package_to_hub`, as we already mentionned in the former units, **you evaluate, record a replay, generate a model card of your agent and push it to the hub**.\n",
        "\n",
        "This way:\n",
        "- You can **showcase our work** üî•\n",
        "- You can **visualize your agent playing** üëÄ\n",
        "- You can **share with the community an agent that others can use** üíæ\n",
        "- You can **access a leaderboard üèÜ to see how well your agent is performing compared to your classmates** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
      ],
      "metadata": {
        "id": "MkMk99m8bgaQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JquRrWytA6eo"
      },
      "source": [
        "To be able to share your model with the community there are three more steps to follow:\n",
        "\n",
        "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n",
        "\n",
        "- Copy the token \n",
        "- Run the cell below and paste the token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZiFBBlzxzxY"
      },
      "outputs": [],
      "source": [
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tsf2uv0g_4p"
      },
      "source": [
        "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGNh9VsZok0i"
      },
      "source": [
        "3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî• using `package_to_hub()` function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "package_to_hub(\n",
        "    model=model,\n",
        "    model_name=f\"a2c-{env_id}\",\n",
        "    model_architecture=\"A2C\",\n",
        "    env_id=env_id,\n",
        "    eval_env=eval_env,\n",
        "    repo_id=f\"JUNGU/a2c-{env_id}\", # Change the username\n",
        "    commit_message=\"Initial commit\",\n",
        ")"
      ],
      "metadata": {
        "id": "ueuzWVCUTkfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take a coffee break ‚òï\n",
        "- You already trained your first robot that learned to move congratutlations ü•≥!\n",
        "- It's **time to take a break**. Don't hesitate to **save this notebook** `File > Save a copy to Drive` to work on this second part later.\n"
      ],
      "metadata": {
        "id": "Qk9ykOk9D6Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÌôòÍ≤Ω 2: PandaReachDense-v2 ü¶æ\n",
        "\n",
        "Ïö∞Î¶¨Í∞Ä ÌõàÎ†®Ìï† ÏóêÏù¥Ï†ÑÌä∏Îäî Î°úÎ¥á ÌåîÏùÑ Ï†úÏñ¥(ÌåîÏùÑ ÏõÄÏßÅÏù¥Í≥† ÏóîÎìú Ïù¥ÌéôÌÑ∞Î•º ÏÇ¨Ïö©)Ìï¥Ïïº ÌïòÎäî Î°úÎ¥á ÌåîÏûÖÎãàÎã§.\n",
        "\n",
        "Î°úÎ¥á Í≥µÌïôÏóêÏÑú ÏóîÎìú Ïù¥ÌéôÌÑ∞Îäî ÌôòÍ≤ΩÍ≥º ÏÉÅÌò∏ÏûëÏö©ÌïòÎèÑÎ°ù ÏÑ§Í≥ÑÎêú Î°úÎ¥á ÌåîÏùò ÎÅùÏóê ÏûàÎäî Ïû•ÏπòÏûÖÎãàÎã§.\n",
        "\n",
        "'ÌåêÎã§Î¶¨Ïπò'ÏóêÏÑú Î°úÎ¥áÏùÄ ÏóîÎìú Ïù¥ÌéôÌÑ∞Î•º Î™©Ìëú ÏúÑÏπò(ÎÖπÏÉâ Í≥µ)Ïóê Î∞∞ÏπòÌï¥Ïïº Ìï©ÎãàÎã§.\n",
        "\n",
        "Ïù¥ ÌôòÍ≤ΩÏùò Î∞ÄÎèÑ Î≤ÑÏ†ÑÏùÑ ÏÇ¨Ïö©Ìï† Í≤ÉÏûÖÎãàÎã§. Ï¶â, Í∞Å ÏãúÍ∞Ñ Îã®Í≥ÑÎßàÎã§ Î≥¥ÏÉÅÏùÑ Ï†úÍ≥µÌïòÎäî *Î∞ÄÎèÑ Î≥¥ÏÉÅ Í∏∞Îä•*ÏùÑ ÏÇ¨Ïö©Ìï† Í≤ÉÏûÖÎãàÎã§(ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏûëÏóÖÏùÑ ÏôÑÎ£åÌïòÎäî Îç∞ Í∞ÄÍπåÏõåÏßàÏàòÎ°ù Î≥¥ÏÉÅÏù¥ ÎÜíÏïÑÏßëÎãàÎã§). ÌôòÍ≤ΩÏù¥ **Í≥ºÏ†úÎ•º ÏôÑÎ£åÌïú Í≤ΩÏö∞ÏóêÎßå Î≥¥ÏÉÅÏùÑ Î∞òÌôò**ÌïòÎäî *Ìù¨ÏÜå Î≥¥ÏÉÅ Ìï®Ïàò*ÏôÄÎäî ÎåÄÏ°∞Ï†ÅÏûÖÎãàÎã§.\n",
        "\n",
        "ÎòêÌïú *ÏóîÎìú Ïù¥ÌéôÌÑ∞ Î≥ÄÏúÑ Ï†úÏñ¥*Î•º ÏÇ¨Ïö©Ìï† ÏòàÏ†ïÏù∏Îç∞, Ïù¥Îäî **ÌñâÎèôÏù¥ ÏóîÎìú Ïù¥ÌéôÌÑ∞Ïùò Î≥ÄÏúÑÏóê Ìï¥Îãπ**ÌïúÎã§Îäî ÏùòÎØ∏ÏûÖÎãàÎã§. Í∞Å Í¥ÄÏ†àÏùò Í∞úÎ≥Ñ ÎèôÏûëÏùÑ Ï†úÏñ¥ÌïòÏßÄ ÏïäÏäµÎãàÎã§(Í¥ÄÏ†à Ï†úÏñ¥).\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/robotics.jpg\" alt=\"Î°úÎ≥¥Ìã±Ïä§\"/>\n",
        "\n",
        "\n",
        "Ïù¥Î†áÍ≤å ÌïòÎ©¥ **ÌõàÎ†®Ïù¥ Îçî Ïâ¨ÏõåÏßëÎãàÎã§**.\n",
        "\n"
      ],
      "metadata": {
        "id": "5VWfwAA7EJg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In `PandaReachDense-v2` the robotic arm must place its end-effector at a target position (green ball).\n",
        "\n"
      ],
      "metadata": {
        "id": "oZ7FyDEi7G3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env_id = \"PandaReachDense-v2\"\n",
        "\n",
        "# Create the env\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape\n",
        "a_size = env.action_space"
      ],
      "metadata": {
        "id": "zXzAu3HYF1WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "id": "E-U9dexcF-FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Í¥ÄÏ∞∞ Í≥µÍ∞Ñ **ÏùÄ ÏÑ∏ Í∞ÄÏßÄ ÏöîÏÜå**Í∞Ä ÏûàÎäî ÏÇ¨Ï†ÑÏûÖÎãàÎã§:\n",
        "- `achieved_goal`: (x,y,z) Î™©ÌëúÏùò ÏúÑÏπò.\n",
        "- `desired_goal`: Î™©Ìëú ÏúÑÏπòÏôÄ ÌòÑÏû¨ Î¨ºÏ≤¥ ÏúÑÏπò ÏÇ¨Ïù¥Ïùò (x,y,z) Í±∞Î¶¨.\n",
        "- `observation`: ÏóîÎìú Ïù¥ÌéôÌÑ∞Ïùò ÏúÑÏπò(x,y,z) Î∞è ÏÜçÎèÑ(vx, vy, vz).\n",
        "\n",
        "Í¥ÄÏ∏°Í∞íÏù¥ ÏÇ¨Ï†ÑÏù¥ÎØÄÎ°ú **MlpPolicy ÎåÄÏã† Îã§Ï§ë ÏûÖÎ†• Ï†ïÏ±Ö**ÏùÑ ÏÇ¨Ïö©Ìï¥Ïïº Ìï©ÎãàÎã§."
      ],
      "metadata": {
        "id": "g_JClfElGFnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "id": "ib1Kxy4AF-FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The action space is a vector with 3 values:\n",
        "- Control x, y, z movement"
      ],
      "metadata": {
        "id": "5MHTHEHZS4yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ïù¥Ï†ú Ïó¨Îü¨Î∂ÑÏùò Ï∞®Î°ÄÏûÖÎãàÎã§:\n",
        "\n",
        "1. \"PandaReachDense-v2\"ÎùºÎäî ÌôòÍ≤ΩÏùÑ Ï†ïÏùòÌï©ÎãàÎã§.\n",
        "2. Î≤°ÌÑ∞ÌôîÎêú ÌôòÍ≤Ω ÎßåÎì§Í∏∞\n",
        "3. Í¥ÄÏ∞∞Í≥º Î≥¥ÏÉÅÏùÑ Ï†ïÍ∑úÌôîÌïòÎäî ÎûòÌçºÎ•º Ï∂îÍ∞ÄÌï©ÎãàÎã§. [Î¨∏ÏÑú ÌôïÏù∏](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)\n",
        "4. A2C Î™®Îç∏ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§(ÌõàÎ†® Î°úÍ∑∏Î•º Ïù∏ÏáÑÌïòÎ†§Î©¥ verbose=1ÏùÑ ÏûäÏßÄ ÎßàÏÑ∏Ïöî).\n",
        "5. 1M ÌÉÄÏûÑÏä§ÌÖùÏúºÎ°ú ÌõàÎ†®Ìï©ÎãàÎã§.\n",
        "6. ÏóêÏù¥Ï†ÑÌä∏Î•º Ï†ÄÏû•Ìï† Îïå Î™®Îç∏ÏùÑ Ï†ÄÏû•ÌïòÍ≥† VecNormalize ÌÜµÍ≥ÑÎ•º Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
        "7. ÏóêÏù¥Ï†ÑÌä∏ ÌèâÍ∞ÄÌïòÍ∏∞\n",
        "8. ÌõàÎ†®Îêú Î™®Îç∏ÏùÑ 'package_to_hub'Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÌóàÎ∏åÏóê Í≤åÏãúÌï©ÎãàÎã§."
      ],
      "metadata": {
        "id": "nIhPoc5t9HjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution (fill the todo)"
      ],
      "metadata": {
        "id": "sKGbFXZq9ikN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - 2\n",
        "env_id = \"PandaReachDense-v2\"\n",
        "env = make_vec_env(env_id, n_envs=4)\n",
        "\n",
        "# 3\n",
        "env = VecNormalize(env,training=True, norm_obs=True, norm_reward=True, clip_obs=10.) #training=TrueÏù¥Í±∞ Ï∂îÍ∞ÄÎ°ú ÌçºÌè¨Î®ºÏä§ Îß§Ïö∞ Ìñ•ÏÉÅÎê®\n",
        "\n",
        "# 4\n",
        "model = A2C(policy = \"MultiInputPolicy\",\n",
        "            env = env,\n",
        "            verbose=1)\n",
        "# 5\n",
        "model.learn(1_000_000)"
      ],
      "metadata": {
        "id": "J-cC-Feg9iMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6\n",
        "model_name = \"a2c-PandaReachDense-v2\"; \n",
        "model.save(model_name)\n",
        "env.save(\"vec_normalize.pkl\")\n",
        "\n",
        "# 7\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "# Load the saved statistics\n",
        "eval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v2\")])\n",
        "eval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n",
        "\n",
        "#  do not update them at test time\n",
        "eval_env.training = False\n",
        "# reward normalization is not needed at test time\n",
        "eval_env.norm_reward = False\n",
        "\n",
        "# Load the agent\n",
        "model = A2C.load(model_name)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "# 8\n",
        "package_to_hub(\n",
        "    model=model,\n",
        "    model_name=f\"a2c-{env_id}\",\n",
        "    model_architecture=\"A2C\",\n",
        "    env_id=env_id,\n",
        "    eval_env=eval_env,\n",
        "    repo_id=f\"JUNGU/a2c-{env_id}\", # TODO: Change the username\n",
        "    commit_message=\"Initial commit\",\n",
        ")"
      ],
      "metadata": {
        "id": "-UnlKLmpg80p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some additional challenges üèÜ\n",
        "The best way to learn **is to try things by your own**! Why not trying  `HalfCheetahBulletEnv-v0` for PyBullet and `PandaPickAndPlace-v1` for Panda-Gym?\n",
        "\n",
        "If you want to try more advanced tasks for panda-gym, you need to check what was done using **TQC or SAC** (a more sample-efficient algorithm suited for robotics tasks). In real robotics, you'll use a more sample-efficient algorithm for a simple reason: contrary to a simulation **if you move your robotic arm too much, you have a risk of breaking it**.\n",
        "\n",
        "PandaPickAndPlace-v1: https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1\n",
        "\n",
        "And don't hesitate to check panda-gym documentation here: https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html\n",
        "\n",
        "Here are some ideas to achieve so:\n",
        "* Train more steps\n",
        "* Try different hyperparameters by looking at what your classmates have done üëâ https://huggingface.co/models?other=https://huggingface.co/models?other=AntBulletEnv-v0\n",
        "* **Push your new trained model** on the Hub üî•\n"
      ],
      "metadata": {
        "id": "G3xy3Nf3c2O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See you on Unit 7! üî•\n",
        "## Keep learning, stay awesome ü§ó"
      ],
      "metadata": {
        "id": "usatLaZ8dM4P"
      }
    }
  ]
}